trainer:
    logger:
        class_path: utils.loggers.wandb.WandbNamedLogger
        init_args:
            project: default_project_name
            save_dir: work_dirs
            offline: false
    callbacks:
        - class_path: utils.progress.rich_progress.RichDefaultThemeProgressBar
          init_args:
              show_version: false
              show_eta_time: true
        - class_path: pytorch_lightning.callbacks.RichModelSummary
          init_args:
              max_depth: 2
        - class_path: utils.callbacks.wandb_logger_watch_model_callback.WandbLoggerWatchModelCallback
          init_args:
              log: all
              log_freq: 50
              log_graph: false
        - class_path: utils.callbacks.wandb_logger_log_all_code_callback.WandbLoggerLogAllCodeCallback
          init_args:
              root: .
              name: null
        - class_path: utils.callbacks.lr_monitor.LearningRateMonitor
          init_args:
              logging_interval: null
              log_momentum: false
        - class_path: utils.callbacks.model_checkpoint.ModelCheckpointWithLinkBest
          init_args:
              monitor: val/loss
              filename: "epoch:{epoch}-val_loss:{val/loss:.4g}"
              save_top_k: 3
              save_last: true
              save_best: true
              mode: min
              auto_insert_metric_name: false
    # debug
    limit_train_batches: 1.0
    limit_val_batches: 1.0
    limit_test_batches: 1.0
    limit_predict_batches: 1.0
    fast_dev_run: false
    overfit_batches: 0.0
    # train
    max_epochs: null
    min_epochs: null
    max_steps: -1
    min_steps: null
    max_time: null
    # k fold cross validation
    num_folds: null
    # gradient clip
    gradient_clip_val: null
    gradient_clip_algorithm: null
    # gpus
    num_nodes: 1
    accelerator: auto
    devices: auto
    strategy: ddp_find_unused_parameters_false
    # speed up
    precision: 32
    auto_lr_find: false
    detect_anomaly: false
    auto_scale_batch_size: false
    accumulate_grad_batches: null
    profiler: null
    # val and log
    check_val_every_n_epoch: 1
    val_check_interval: 1.0
    log_every_n_steps: 50
    track_grad_norm: -1

# seed
seed_everything: null
